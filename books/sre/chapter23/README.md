# CHAPTER 23. Managing Critical State: Distributed Consensus for Reliability

신뢰할 수 있는 고가용성 시스템을 효과적으로 구축하기 위해 분산에 대한 합의가 필요

분산 시스템의 여러 프로세스들은 중요한 설정을 일관되게 확인할 수 있어야 함

<br><img src="./img/figure23-1.png" width="80%" /><br>

분산 환경에서 합의 시스템은 충분히 테스트되고 증명된 시스템을 권장

<br/>

**저장소**
- 시스템, 소프트웨어 저장소: ACID 기반
  - Atomicity, Consistency, Isolation, Durability
  - 원자성, 영속성, 격리성, 내구성
- 분산 데이터 저장소: BASE 기반
  - Basically Available, Soft state, Eventual consistency
  - 기본적인 가용성, 유연한 상태, 궁극적 일관성
  - 보통 멀티마스터 복제(multimaster replication) 지원: 여러 프로세스에서 쓰기 작업을 동시에 수행할 수 있음

<br/>

## Motivating the Use of Consensus: Distributed Systems Coordination Failure

<small><i>합의는 왜 필요할까: 분산 시스템 간 협업의 실패</i></small>

### Case Study 1: The Split-Brain Problem

<small><i>사례 연구 1: 스플릿 브레인 문제</i></small>

- 살펴볼 서비스: 여러 사용자들이 협업할 수 있는 콘텐츠 저장소
- 신뢰성을 위해, **서로 다른 랙에 배치된 두 개의 복제된 파일 서버** 사용
- 한 쌍의 파일 서버 중 하나는 **리더**(leader), 다른 하나는 **수행 서버**(follower) 역할 수행
- 장애 발생 시 **STONITH**를 통해 리더십 확보

<pre><b>STONITH</b>
Shoot The Other Node in the Head.
상대 서버를 강제로 종료.
</pre>

#### 💥 **문제점**
- 네트워크 지연이나 패킷 손실 발생 시, 서버들이 서로를 인식하지 못하고 동시에 **STONITH** 명령을 실행할 가능성 존재
- 일부 명령이 네트워크 문제로 전달되지 않아, 두 서버가 동시에 활성 상태이거나 둘 다 종료될 수 있음
  - 데이터 손상(동시 쓰기) 또는 가용성 문제(둘 다 종료됨) 발생하게 됨
- 단순한 타임아웃을 통한 리더 선출 방식은 본질적으로 분산 비동기 합의 문제를 해결할 수 없음

<br/>

### Case Study 2: Failover Requires Human Intervention

<small><i>사례 연구 2: 인간 개입이 필요한 장애 조치</i></small>

- 다중 샤드 데이터베이스 시스템에서 각 샤드의 주(primary) 노드는 다른 데이터센터의 보조(secondary) 노드로 동기 복제 수행
- 외부 시스템이 기본 노드 상태를 점검하고, 문제가 발생하면 보조 노드를 승격
- 기본 노드가 보조 노드 상태를 확인할 수 없을 경우, 스스로 비활성화하고 인간 개입 요청

#### 💥 **문제점**
- 데이터 손실 위험은 없으나, **가용성이 안좋음**
  - 시스템 운영 엔지니어의 업무 증가
  - 확장성 부족
- 네트워크의 심각한 문제로 리더를 선출하지 못하면, 인간 개입이 더 나은 것도 아님

<br/>

### Case Study 3: Faulty Group-Membership Algorithms

<small><i>사례 연구 3: 잘못된 그룹-멤버십 알고리즘</i></small>

- 인덱싱을 수행하고 검색 서비스를 제공하는 시스템 운영
- 노드들은 시작 시 **가십 프로토콜(gossip protocol)** 을 사용해 서로를 인식하고 클러스터에 가입
- 클러스터 내에서 리더를 선출하고 조정 역할 수행

#### 💥 **문제점**
- 네트워크 문제 시, 클러스터가 분리되고, **스플릿 브레인 상태**가 발생하여 데이터 손상 초래
- 그룹 멤버십을 일관되게 유지하는 문제 역시 분산 합의 문제의 한 형태

분산에 대한 합의 알고리즘은 정확성이 입증됨, 그 구현의 **포괄적으로 테스트된 분산에 대한 합의 알고리즘을 통해서만** 해결될 수 있음

<br/>

## How Distributed Consensus Works

<small><i>분산 합의 동작 방식</i></small>

**분산 합의 알고리즘 방식** 
- **충돌-실패** (crash-fail): 충돌이 발생한 노드를 시스템에서 영구 제외 
- **충돌-복구** (crash-recover)

**분산 합의 구분**
- **Asynchronous distributed consensus (비동기 분산 합의)**
- **Synchronous distributed consensus (동기 분산 합의)**

**분산 합의 알고리즘 방식**
- **Crash-Fail (충돌-실패)**
  - 장애가 발생한 노드는 영구 제외
- **Crash-Recover (충돌-복구)**
  - 장애 발생 후 복구 가능
  - 실무에서 더 유용함 (네트워크 지연, 재시작 등의 일시적 문제 대응 가능)

**장애 종류**
- **Byzantine Failure (비잔틴 장애)**
  - 악의적이거나 버그로 인해 잘못된 메시지를 전달하는 경우
  - 처리 비용이 높고 발생 빈도가 높지 않음
- **Non-Byzantine Failure (비비잔틴 장애)**
  - 단순한 충돌이나 네트워크 지연 등의 문제

**분산 합의 알고리즘**
- **Paxos (Lamport, 1998)**
- **Raft (2014)**
- **Zab (2011)**
- **Mencius (2008)**

<br/>

### Paxos Overview: An Example Protocol

<small><i>Paxos 살펴보기 : 예제 프로토콜</i></small>

- Paxos는 대부분의 프로세스가 제안을 수용할 수 있는지 여부에 따라 제안 시퀀스로 작동.
  - 다수의 프로세스가 제안을 수용하는지 확인하는 프로토콜.
  - 제안이 수용되지 않으면 실패함.
  - 각 제안은 순서를 갖는 고유한 시퀀스 번호를 가짐.

**동작 과정**

1. **Prepare**
- 제안자(Proposer)가 순서 번호를 선택하여 수락자(Acceptor)에게 전달
- 수락자는 더 높은 순서 번호를 가진 제안을 본 적이 없다면 동의
2. **Promise**
- 수락자가 과거에 본 적 없는 높은 번호의 제안에만 동의
- 제안자가 과반수의 동의를 얻으면 다음 단계로 진행 가능
3. **Accept**
- 다수의 수락자가 승인하면 제안자는 해당 값을 커밋
- 수락자는 동의한 제안을 **저널(로그)** 에 기록하여 장애 후에도 일관성 유지

**장점**
- 시퀀싱으로 메시지 순서 문제 해결.
- 과반수 동의로 제안에 대해 두 개의 값 커밋 방지.
- 수락자는 제안 수락 시 저널을 작성하여 재시작 시에도 보증 유지.

**단점**
- Paxos는 한 번의 값과 제안 번호 합의만 가능.
- 특정 노드는 합의된 값의 전체 세트를 보지 못할 수 있음.

<br/>

## System Architecture Patterns for Distributed Consensus

<small><i>분산 합의를 위한 시스템 아키텍처 패턴</i></small>

- **분산 합의 알고리즘**
  - 저수준(ow-leve) & 원시적(primitive)
 
- 노드 집합이 값에 단 한 번 동의하게 함.
  - 높은 수준의 시스템 구성 요소를 추가하여 실용적인 시스템 기능 제공해야함.
    - → 설계 복잡성 감소 및 합의 알고리즘의 교체 용이.

- **합의 알고리즘을 구현하는 서비스:**
    - Zookeeper, Consul, etcd와 같은 서비스가 많이 사용됨
    - Zookeeper와 비슷하게 구글의 Chubby가 있음
    - 합의에 대한 근본 기능를 서비스로 제공하면, 엔지니어가 고가용성 서비스와의 호환성을 유지해야 하는 부담 감소.

<br>
    
### Reliable Replicated State Machines

<small><i>신뢰할 수 있는 복제된 상태 머신</i></small>

- **RSM**
  - Replicated state machine.
  - **여러 프로세스**에서 **동일한 작업**을 **동일한 순서**로 실행.
  - 합의 알고리즘을 통해 전역적으로 순서가 정해짐.
  - 상태 동기화를 위해 **슬라이딩 윈도우 프로토콜(sliding-window protocol)** 사용 가능.

<br/>
<img src="./img/figure23-2.png" width="100%">
합의 알고리즘과 복제된 상태 머신 사이의 관계
<br/>

<br>

### Reliable Replicated Datastores and Configuration Stores

<small><i>신뢰할 수 있는 복제된 데이터와 설정 저장소</i></small>

**Reliable replicated datastores**
- 복제된 상태 머신 애플리케이션
- 중요한 작업 시, 합의 알고리즘 사용
  - 디자인 단계에서 성능, 처리량, 확장성 고려가 중요.
- 비분산 합의 기반 시스템은 단순히 리턴되는 데이터의 수명을 제한하기 위해 타임스탬프(timestamp)에 의존
  - 해결: **Spanner**
    - 타임스탬프에 의존하는 시스템의 문제 해결 가능.
    - 불확실성을 해결할 필요가 있는 프로세스의 실행을 최대한 늦춤

<br/>

### Highly Available Processing Using Leader Election

<small><i>리더 선출을 통한 고가용성 처리</i></small>

- 분산 시스템에서 리더 선출은 분산 합의와 유사
- **리더 선출:**
- 단일 리더를 사용하여 특정 작업 수행.

<br/>
<img src="./img/figure23-3.png" width="70%">

마스터 선출을 위해 복제된 서비스를 사용하는 고가용성 시스템
<br/>

- 리더가 작업자(Worker) 풀을 관리하는 작업 수행.
  - 중요한 작업에 합의 알고리즘을 사용하지 않아, 일반적으로 처리량을 중요하게 생각하진 않음.


<br/>

### Distributed Coordination and Locking Services

<small><i>분산 조정 및 잠금 서비스</i></small>

- **Barrier (장벽):**
  - 특정 조건이 충족될 때까지 프로세스 그룹을 차단.
    - e.g. MapReduce: Map 단계가 완료되기 전까지 Reduce 연산을 막음
  - Zookeeper의 구현 원리임.

<br/><img src="./img/figure23-4.png" width="100%">
맵리듀스 계산을 위한 프로세스 조율이 장벽
<br/>

- **Locks (잠금):**
  - 워커 프로세스의 원자적 작업 보장.
  - 갱신 가능한 Lease System 사용 권장.
    - 일정 시간 후 잠금을 다시 임대하게 함.
    - 무기한 잠금 방지
  - 분산 잠금은 저수준 시스템에서는 유의해서 도입해야할 기본적인 개념.
  - 대부분의 애플리케이션은 분산 트랜잭션을 제공하는 고수준의 시스템을 이용해야 함.

<br/>

### Reliable Distributed Queuing and Messaging

<small><i>신뢰할 수 있는 분산 큐잉 및 메시징</i></small>

- **Queues**:
  - 주로 여러 작업자 프로세스간의 분산 작업을 위해 사용
  - Lease System 사용 권장.

- **Queuing-based systems**
  - 큐 손실 시 전체 시스템이 동작하지 않을 수 있어서, RSM으로 구현하여 위험 최소화.
  
- **Atomic broadcast** (원자적 브로드캐스트)
  - **메시지의 안정적이면서 동일한 순서 전달** 보장.
  - Publish-Subscribe (발행-구독 시스템)을 활용할 수 있음

<br/><img src="./img/figure23-5.png" width="100%" />
신뢰할 수 있는 합의 기반 큐 컴포넌트를 사용하는 큐 지향 작업 분산 시스템<br/>

**Queuing-as-work-distribution pattern**
- 로드밸런싱 장치에서 큐를 사용하는 기법.
- 종단간(point-to-point) 메시징에 유용.
- Publish-Subscribe 제공: 채널 또는 주제에 구독하는 다수의 클라이언트에게 메시지 제공.
- 큐에서 태스크를 받아가야 하는 시스템에서 지연 응답이 높아지면, 각 태스크의 처리 시간 비율이 급격히 증가해서 문제가 발생할 수 있음.

<br/>

## Distributed Consensus Performance

<small><i>분산 합의의 성능</i></small>

- **일반 통념:**
  - 많은 시스템에서 합의 알고리즘은 느리고 비용이 많이 든다고 여겨짐.
  - 하지만 여러 가지 방법으로 성능을 향상시킬 수 있음.
  - 구글의 중요한 시스템에서 합의 알고리즘이 매우 효과적으로 사용됨.

- **성능에 영향을 미치는 요소:**
  - **작업 부하:**
    - 처리량: 최대 부하 시, 단위 시간당 제안 수.
    - 요청 종류: 상태를 변경하는 작업 비율.
    - 읽기 작업에 필요한 일관성.
    - 요청 크기: 데이터 페이로드 크기의 다양성.
  - **배포 전략:**
    - 로컬 영역 또는 전역 영역 배포 여부.
    - 사용된 쿼럼 알고리즘 및 대부분의 프로세스 위치.
    - 샤딩, 파이프라이닝, 배칭 (sharding, pipelining, batching) 사용 여부.

- **리더 프로세스 사용:**
    - 모든 요청을 리더 노드로 보내야 하므로, 지리적 위치에 따라 성능 차이 발생.

<br/><img src="./img/figure23-6.png" width="100%" />
서버 프로세스로 부터의 거리에 따른 클라이언트의 지연 응답의 변화
<br/>


### Multi-Paxos: Detailed Message Flow

<small><i>Multi-Paxos: 메시지 흐름</i></small>

- Multi-Paxos 프로토콜은 Strong Leader Process (강한 리더 프로세스) 사용
  - : 단 한 번의 라운드 트립만 허용.

<br/><img src="./img/figure23-7.png" width="100%" />
Multi-Paxos 프로토콜의 기본적인 메시지 흐름
<br/>

1. **Prepare** 단계:
- 제안자(Proposer)는 고유 번호를 가진 준비 메시지(Prepare)를 보냄
- 수락자(Acceptor)는 받은 준비 메시지를 기록하고, 그 고유 번호가 자신이 이전에 기록한 어떤 제안보다 큰 경우, 응답 메시지(Promise)를 보냄.
  - 이때, 수락자는 동일한 제안자에 대한 나중 제안을 거부할 것이라고 약속함.
- **Promise** 단계:
  - 수락자(Acceptor)는 제안자(Proposer)에게 Promise 응답 메시지를 보냅니다. 이는 프로포저에게 자신이 준비 메시지를 받아들였다는 것을 알려줍니다.
- **Propose** 단계:
  - 제안자가 수락자 중 다수에게서 Promise를 받으면, 제안자는 최종 제안(Accept)을 만들어 보냄.
  - 이 때 제안(Accept)은 준비된 값 및 고유 번호를 포함함.
- **Accept** 요청:
  - 제안자는 제안된 값을 수락해 달라는 요청인 `Accept` 메시지를 보냄.
  - 수락자는 이미 보낸 약속(Promise)에 따라 값을 수락하거나 거부.
- **Learn** 단계:
  - 수락자들이 다수 결정을 내리면, 합의된 값을 제안자 및 학습자에게 전달.
  - 학습자는 값을 받아 이해하고 외부 시스템에 전달.


제안자가 변경될 경우 추가 왕복 단계가 필요하며 성능 저하 가능.

- **충돌 방지:**
  - 충돌 방지를 위해 프로세서 할당 또는 회전하는 제안자 시스템 사용.

**라이브록(ivelock)**
- 제안이 계속 서로 방해해서 어떤 제안도 수락될 수 없다면 제안자가 둘이 되는 상황이 만들어질 수 있음

<br/><img src="./img/figure23-8.png" width="100%" />
Multi-Paxos 프로토콜의 이중 제안자 현상
<br/>

해결법  
- 모든 합의 시스템 아래 방법 중 하나 선택
  - 제안 프로세스를 선출: 시스템 내의 모든 제안을 담당하는 제안 프로세스를 선출
  - 제안자 회전 방식: 각 프로세스들이 제안을 할 수 있는 특정 슬롯을 할당하는 제안자 회전(rotating) 방식.

- **리더 프로세스 주의점**: 리더가 없을 때 불균형이 발생할 수 있으며, 이중 제안자 현상이 나타날 우려가 있음.
- 적절한 타임아웃과 백오프 (backoff) 전략을 구현하는 것이 중요.
- Raft는 리더 선출 프로세스에 대해 숙고하고 구현한 방법을 제공함.

<br/>

### Scaling Read-Heavy Workloads

<small><i>읽기 작업 부하의 확장</i></small>

- 읽기 부하 확장은 중요. 많은 작업이 읽기 중심(read-heavy)이기 때문.
- 많은 작업이 읽기 중심일 경우, 복제 서버에서 데이터 읽기 가능.

**일관성 있는 읽기 필요 시**
- 읽기 전용 합의 작업 수행.
- 가장 최신의 데이터를 보장하는 복제 서버로 부터 데이터 읽기
  - 가령, 안정적인 리더 복제본에서 읽기를 수행
- Quorum Leases(과반수 임대) 사용.
  - 일부 복제 서버가 시스템의 전체 혹은 일부 데이터에 대한 임대하고 약간의 쓰기 성능을 희생해서 읽기 작업에 대한 강력한 일관성을 제공하는것

<br/>

### Quorum Leases

<small><i>정족수 임대</i></small>

- 분산 합의 성능 최적화 기법. 
  - 읽기 작업에 대한 지연용답을 최소화하고 처리량을 증가시키는 것을 목표.
- 일정(짧은) 기간 동안 데이터 상태에 대해 읽기 리스를 쿼럼에 부여.
- 데이터 상태 변경 작업은 모두 복제 서버들의 동의가 필요.
- 일부 복제본 사용 불가 시, 리스가 만료될 때까지 데이터 수정 불가.
- 읽기 중심 작업에 유용.

<br/>

### Distributed Consensus Performance and Network Latency

<small><i>분산 합의 성능과 네트워크 지연 응답</i></small>

- **네트워크 지연:**
  - 네트워크 왕복 시간 및 지속 저장소 쓰기 시간에 성능이 크게 영향을 받음.
  - 네트워크 정체와 물리적 거리 모두 성능에 영향.
  - TCP/IP 통신을 사용하는 합의 시스템 일반적.
    - TCP/IP는 핸드셰이크와 연결 유지 비용이 비교적 크게 듦
- 지역 네트워크 내 합의 시스템 성능은 데이터베이스의 비동기 리더-팔로워 복제 시스템만큼 우수.
  - 하지만, 가용성을 위한 분산 합의 시스템은 장애 격리를 위해 '멀리 떨어진' 복제 서버를 필요로 함.

<br/><img src="./img/figure23-9.png" width="100%" />
클라이언트의 지역 간 TCP/IP 연결에 대한 수요를 줄이기 위해 프록시를 사용하는 방법
<br/>

- 영속적 TCP/IP 연결을 가지고 있는 지역 별 프록시 풀 사용
  - 프록시를 사용해 클라이언트가 원거리 TCP/IP 연결 오버헤드를 줄일 수 있음

<br/>

### Reasoning About Performance: Fast Paxos

<small><i>Fast Paxos</i></small>

- **Fast Paxos 프로토콜:**
  - 클라이언트가 직접 모든 수락자에게 제안 메시지를 보냄.
  - 지연 시간이 긴 경우 Multi-Paxos보다 느릴 수 있음.
  - 작업 처리를 병렬로 보내 성능 최적화 가능.

<br/>

<br />
